# AI Summarizer + MCP (Windows Desktop)

á»¨ng dá»¥ng desktop cháº¡y ná»n (system tray) cho Windows:
- BÃ´i Ä‘en vÄƒn báº£n â†’ **Shift + chuá»™t pháº£i** â†’ panel hÃ nh Ä‘á»™ng.
- Gá»i **LLM cá»¥c bá»™** (Ollama hoáº·c LM Studio) Ä‘á»ƒ **tÃ³m táº¯t/giáº£i thÃ­ch/dá»‹ch**.
- **TÃ­ch há»£p MCP**: chá»n server/tool, cháº¡y tool (filesystem, web, DBâ€¦), láº¥y káº¿t quáº£ lÃ m **ngá»¯ cáº£nh** trÆ°á»›c khi tÃ³m táº¯t.

## TÃ­nh nÄƒng
- **Chat Window**: Chat vá»›i AI, há»— trá»£ lá»‹ch sá»­ há»™i thoáº¡i vÃ  tÃ­ch há»£p MCP tools thá»§ cÃ´ng.
- Popup hÃ nh Ä‘á»™ng cáº¡nh con trá»: TÃ³m táº¯t, Giáº£i thÃ­ch, Dá»‹ch, Viáº¿t láº¡i, Prompt tuá»³ biáº¿n.
- Provider: **Ollama** (`/api/generate` & `/api/chat`) hoáº·c **LM Studio** (OpenAI-compatible `/v1/chat/completions`).
- **MCP Panel**: liá»‡t kÃª servers/tools, nháº­p args JSON, cháº¡y tool vÃ  chÃ¨n káº¿t quáº£ vÃ o prompt/chat.
- Báº£o máº­t: Máº·c Ä‘á»‹nh chá»‰ gá»i endpoint **local** vÃ  MCP servers cá»¥c bá»™.

## YÃªu cáº§u há»‡ thá»‘ng
- **Windows 10/11**
- Python 3.10+ (khuyáº¿n nghá»‹ 64-bit)
- Node.js (Ä‘á»ƒ cháº¡y MCP filesystem server qua `npx`)

## CÃ i Ä‘áº·t
```bash
# 1) Táº¡o venv vÃ  cÃ i thÆ° viá»‡n
python -m venv .venv
.venv\Scripts\activate
pip install PySide6 pynput pywin32 requests "mcp[cli]" python-dotenv

# 2) (Tuá»³ chá»n) ÄÃ³ng gÃ³i .exe
pip install pyinstaller
pyinstaller -F -w app.py
```

> **Gá»£i Ã½ model cá»¥c bá»™** (mÃ¡y i7-14700K, RAM 64GB): `llama3.2:3b-instruct`, `mistral:7b-instruct` (Q4), `phi3.1:3.8b`. Ollama nhanh & Ä‘Æ¡n giáº£n.

## Cáº¥u hÃ¬nh
Láº§n cháº¡y Ä‘áº§u sáº½ táº¡o `config.json`. Báº¡n cÃ³ thá»ƒ sá»­a qua menu **Cáº¥u hÃ¬nhâ€¦** hoáº·c trá»±c tiáº¿p file.
```json
{
  "provider": "ollama",
  "ollama": {
    "endpoint": "http://127.0.0.1:11434",
    "model": "llama3.2:3b-instruct",
    "temperature": 0.2,
    "max_tokens": 1024
  },
  "lmstudio": {
    "endpoint": "http://127.0.0.1:1234/v1",
    "model": "Meta-Llama-3.1-8B-Instruct-Q4_K_M",
    "temperature": 0.2,
    "max_tokens": 1024
  },
  "ui": {
    "summary_language": "vi",
    "trigger": {"modifier": "shift", "button": "right"}
  },
  "mcp": {
    "enabled": true,
    "servers": [
      {
        "name": "filesystem",
        "command": "npx",
        "args": ["@modelcontextprotocol/server-filesystem"],
        "env": {"ROOT_PATH": "C:/Users/YourUser/Documents"}
      }
    ]
  }
}
```
> **LÆ°u Ã½**: DÃ¹ng **forward slash** `/` hoáº·c escape backslash `\\` trong JSON Ä‘Æ°á»ng dáº«n.

## Cháº¡y providers
### Ollama
```bash
# CÃ i vÃ  cháº¡y server
# https://ollama.com 
ollama serve
ollama pull llama3.2:3b-instruct
```
### LM Studio
- Má»Ÿ LM Studio â†’ Tab **Server** â†’ chá»n model â†’ **Start Server** (máº·c Ä‘á»‹nh `http://127.0.0.1:1234/v1`).

## Cháº¡y MCP servers
VÃ­ dá»¥ **Filesystem server** qua `npx` (Node.js):
- á»¨ng dá»¥ng sáº½ tá»± khá»Ÿi cháº¡y server qua cáº¥u hÃ¬nh stdio.
- Báº¡n cÃ³ thá»ƒ thay `ROOT_PATH` Ä‘á»ƒ giá»›i háº¡n pháº¡m vi Ä‘á»c/ghi.

## Sá»­ dá»¥ng
1. Cháº¡y `python app.py` (hoáº·c má»Ÿ `dist/app.exe`).
2. Biá»ƒu tÆ°á»£ng mÃ¡y tÃ­nh xuáº¥t hiá»‡n á»Ÿ **system tray**.
3. BÃ´i Ä‘en vÄƒn báº£n â†’ giá»¯ **Shift** â†’ **Click chuá»™t pháº£i**.
4. Chá»n hÃ nh Ä‘á»™ng: TÃ³m táº¯t / Giáº£i thÃ­ch / Dá»‹ch / Viáº¿t láº¡i / Prompt.
5. Äá»ƒ thÃªm ngá»¯ cáº£nh tá»« MCP:
   - Má»Ÿ menu tray â†’ **MCP: Panel chá»n server/tool**.
   - Chá»n **server**, **tool**, nháº­p **args JSON** â†’ **Cháº¡y tool**.
   - Tick **â€œDÃ¹ng káº¿t quáº£ lÃ m ngá»¯ cáº£nh tÃ³m táº¯tâ€** â†’ **ÄÃ³ng**.
   - Thá»±c hiá»‡n tÃ³m táº¯t nhÆ° bÆ°á»›c 3â€“4.
6. **Sá»­ dá»¥ng Chat Window**:
   - Má»Ÿ menu tray (chuá»™t trÃ¡i hoáº·c pháº£i) â†’ chá»n **ğŸ’¬ Má»Ÿ Chat**.
   - Chat bÃ¬nh thÆ°á»ng vá»›i AI.
   - Äá»ƒ dÃ¹ng MCP tools:
     - Click nÃºt **ğŸ“ MCP Tools** trong cá»­a sá»• chat.
     - Chá»n tool vÃ  cháº¡y â†’ káº¿t quáº£ sáº½ tá»± Ä‘á»™ng thÃªm vÃ o Ä‘oáº¡n chat dÆ°á»›i dáº¡ng "Tool Result".
     - AI sáº½ dÃ¹ng thÃ´ng tin Ä‘Ã³ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i tiáº¿p theo cá»§a báº¡n.
   - CÃ¡c tÃ­nh nÄƒng khÃ¡c: Clear history, Export chat to .txt.

## ÄÃ³ng gÃ³i .exe
```bash
pyinstaller -F -w app.py
# File táº¡o táº¡i dist/app.exe
```

## ThÆ° má»¥c dá»± Ã¡n
```
ai_summarizer_mcp/
â”œâ”€ app.py
â”œâ”€ README.md
â”œâ”€ requirements.txt
â””â”€ (tá»± sinh) config.json sau láº§n cháº¡y Ä‘áº§u
```

## Troubleshooting
- **KhÃ´ng copy Ä‘Æ°á»£c selection**: Má»™t sá»‘ app cháº·n `Ctrl+C`. HÃ£y nháº¥n `Ctrl+C` thá»§ cÃ´ng trÆ°á»›c rá»“i dÃ¹ng gesture.
- **Ollama 404/timeout**: Kiá»ƒm tra `ollama serve` vÃ  model Ä‘Ã£ `pull`.
- **MCP server khÃ´ng hiá»‡n tools**: Kiá»ƒm tra Node.js, thá»­ cháº¡y thá»§ cÃ´ng `npx @modelcontextprotocol/server-filesystem`.
- **LM Studio tráº£ lá»—i**: Äáº£m báº£o endpoint `/v1` vÃ  model Ä‘Ãºng tÃªn.

## Ghi chÃº & chuáº©n MCP
- MCP lÃ  chuáº©n má»Ÿ (Anthropic) cho tools/resources/prompts; SDK Python há»— trá»£ clientâ€“server vÃ  stdio/HTTP/SSE transport.
- Tham kháº£o: Model Context Protocol Python SDK & hÆ°á»›ng dáº«n Build an MCP client.

